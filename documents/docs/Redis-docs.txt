Redis is an open source, in-memory data structure store, used as database, cache and message broker.
it is consider to be NoSQL Key/Value Store

Redis is similar to MongoDB in that it is NoSQL and could be used for similar key/value cases.
MongoDB is a "disk-based" document store while Redis is "memory-based"

so Redis is generally faster but MongoDB is little better for prototyping
and it's a little more flexible.

Redis is also used for caching.
It is like Memcached. Memcached also stores data in memory, but one of the big differences here is
that Memcached uses volatile cash. therefor the data does not get persisted and it's lost with a restart.
Redis uses built-in persistence and will not disappear with a restart.
it will actually be saved like a traditional database.

Data Types: Strings, Hashes, Lists, Sets, Sorted Sets, Bitmaps, Hyperlogs, Geospatial, Indexes.

Scaling & Partitioning:
When it comes to scaling, it is not the easiest thing to do compared to some systems but it has
gotten substantially better recently.
If Redis is being used as a cache, scaling up and down using consistent hashing is relatively easy.
However if used is a store a fixed key to nodes map os used so that the number of nodes must be fixed and
cannot vary which obviously can cause some difficulty in scaling.

we can partition which is to split the data among multiple Redis instances and that will allow much larger databases
using the some of the memory of many different computers.
without partitioning, you're limited to the amount of memory that a single computer can support.

Security:
Redis is designed to be accessed by trusted clients inside of trusted enviroments.
it means that if we expose Rdeis to the Internet or untrusted networks, it's not a good idea.
We're talking about using Redis with a web application, than the application should mediate the access between Redis and the Internet.
So Redis is not optimized for maximum security but for maximum performance and simplicity.
you can use Redis with pretty much any application.
if you go to http://redis.io/clients , you will find a list of all available clients for each language.
it also give you recommended clients to use for that language.

Data Structure Server:
Redis is defined as a data structure server or data structure store rather than just a hard database.
so it essentially serves you data that has been stored in a structured way and this does not serve raw data like in SQL database would.
When you work with relational database systems you have to plan everything out before you do anything.
you need to create tables, fields, data types, pretty much everything.
In Redis you don't necessarily need to create schemas and call them names and so on, but you do need to figure out 
how your data will be represented. Will it be Sets or Store Sets or Hashes and so forth
one of the biggest advantages of Redis is speed. but in order to utilize that speed you need to have proper data structure.
whether it's going to be a set or a list or a hash.

Difference between traditional database and Redis:
In a traditional relational database, you have two main methods for getting data from it.
you run a query that scans an entire table or you can make it a little faster by creating an index and having index scanned.
so your SQL gets converted into internal database retrieval or table scanning.
In Redis you don't need an internal query engine because you run data retrieval commands directly.
And the lack of a query engine means that you can't just mindlessly(bi mahaba) store data and think about retreval later. that's more an SQL mindstate.
In Redis you want to decide how to store and retrieve you data beforehand.
So we're not necessarily thinking about the actual data and the types like you would with SQL, but how we're
going to get it and how we're going to retrieve it and send it to the application.

Some questions we may want to ask when we're creating our database and When we're deciding the structure to go with.
Do our values need Keys? if yes, you may need sth like Hash => a list of groups of strings. kind of like One dimentional array.
Dou you need multiple fields to one key?
Do you want to count elements under one collection?
Are you storing objects? if yse you probably want to use a hash
Do you need increasing or decreasing order?
Do you need unique values?
Do you want to test if values exist or not? if so, you probably need to use Set or Stored Set.

Custom Indexing:
Every project has certain ways to get data. if you want to fetch data with more than one retreval key, 
then you may want to look into creating a custom index.

let's say we have a user stored in hashes.and we set up the key to be a user and then useId as an index.
we might want to be able to fetch those users by name as well.
so for that we'll set up a custom index that uses the format User and then the user's name.
so we explicitly have to set that and maintain it ourselves.
-----------------------------------------------------
User:[UserId]
User:[Name]

GET UserName:[Name]
-----------------------------------------------------
you don't have to do this, you could get the users name using NHS scan or an H scan command over all 
the hashes and then you would read the name of the hash where that record is and find match.
you could do that but it's more difficult and it's a little more taxing on system than it needs to be.

Data Storage:
it is different from SQL
Redis was created for explicit reading, not for add whole querying like SQL.
data is stored in live memory so data that you don't even know if you're going to use it is pretty wasteful 
and can be quite inefficient. if you find yourself doing that, then Redis probably wasn't the right solution for that application.

Security Model:
Redis designerd to be accessed by trusted clients
so no outside access by the internet or untrusted networks. (Do not allow any external access if possible)
Simple authentication can be setup, but there is no internal access control.
you can also restrict Redis to certain interfaces which will go over as well.

Network Security:
access to the right port should always be denied to all but trusted clients on the network.
you don't want to open it up to the internet. Only the server is running Redis should be able to directly access it.
(Deny access to main Redis port)
now if you're using it for virtualise Linux instance or some other single computer that's directly connected to the Internet,
you want to avoid that. But if that is the situation than the Redis port should be properly firewalled to prevent outside access.
And the loopback can be used so we can bind 127.0.0.1 as a single interface using the config file (redis.conf).

Authentication:
Redis doesn't have any advanced access control capabilities, but it does have a tiny authentication layer.
this can be anabled in redis.conf ,and when it's turend on, it will refuse any unauthenticated clients.
so the user must send the auth command along with the working password in orderr to make any type of query.
password is stored in plain text in the redis.conf file. so it's really suggested to make the password very very strong because it really doesn't matter
if you forget or if you lose it because it's in plain text anyways.

Data Encryption:
unfortunately Redis doesn't have any kind of data encryption
if for any reason Redis is open to the Internet then it suggested that you use some kind of additional protection layer
such as SSL proxy. Redis documentaion suggests a program called Spiped which involve some kind of symmetrical data encryption.

another security measure: Disabling & Renaming Commands which can do a lot of harm to your system:
because we don't want normal users to be able to run the redis.conf or the flush all command. 
we can disable or rename commands from the redis.conf file.
-----------------------------------------------------
rename-command CONFIG b23hreo238yrblekwuhdels8fydeiwlydewfdow8erl3  // renaming command
rename-command CONFIG ""    // disabling command = rename it to nothing
-----------------------------------------------------

there are some other ways that hackers could harm the system even if you only allow trusted clients to connect.
like hackers being able to insert data triggers some kind of harmful algorithm on the data structure. attacker can insert
bunch of strings in web application form and can overload the CPU and cause DOS (denial of service attack).

Installing:
first install, update and upgrade homebrew
Install Homebrew:
---------------------------
/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
brew update
brew upgrade
---------------------------
then install redis based "https://medium.com/@petehouston/install-and-config-redis-on-mac-os-x-via-homebrew-eb8df9a4f298"
command: $ brew install redis
---------------------------
it's a good practice to copy the configuration file as a default conf file:
$ cp /usr/local/etc/redis.conf /usr/local/etc/redis.conf.default
---------------------------
run all commands of "https://medium.com/@petehouston/install-and-config-redis-on-mac-os-x-via-homebrew-eb8df9a4f298"
and we're ready to go. you can write:
$ redis-server

---------------------------------------------------
if it gave error, do as:
https://stackoverflow.com/questions/32947076/redis-server-in-ubuntu14-04-bind-address-already-in-use
---------------------------------------------------

to start Redis server on background. on port: 6379
then we can run Redis client by:
$ redis-cli
now if we write ping, it returns pong
---------------------------
127.0.0.1:6379> ping
PONG
---------------------------
in order to enter Redis command, we should write $ redis-cli
now if we want to set a key/value pair we use SET:
---------------------------
127.0.0.1:6379> SET foo 100
OK
---------------------------
if we want to get that value, we use GET
---------------------------
127.0.0.1:6379> GET foo
"100"
---------------------------
INCR is for increment and DECR is for decrement
---------------------------
127.0.0.1:6379> INCR foo
(integer) 101       // it's type is changed from string to integer
---------------------------
EXISTS is for seeing if there is a value for specified key
---------------------------
127.0.0.1:6379> EXISTS bar
(integer) 1
127.0.0.1:6379> EXISTS bar2
(integer) 0
---------------------------
DEL for deleting
---------------------------
127.0.0.1:6379> DEL bar
(integer) 1
---------------------------
when ever if we even are not in actual client, we can run commands with redis-cli:
---------------------------
$ redis-cli ECHO Hello
"Hello"
---------------------------
we can write our output to a file if we want. with writing ">":
---------------------------
% redis-cli INCR foo > commands.txt
---------------------------
by writing it, you can see a file name commands.txt will be shown in Home directory of system. with value of 102

we can also monitor all the activity:
so if we enter the redis-cli in another terminal window, and run:
$ redis-cli monitor
whatever we do in the first redis-cli, will be monitored.
so if in first terminal we write:
---------------------------
127.0.0.1:6379> INCR foo
(integer) 103
---------------------------
in the other terminal we have:
---------------------------
1582834636.817776 [0 127.0.0.1:65367] "INCR" "foo"
---------------------------

another command is FLUSHALL which is kinda a cache clear:
it get rid of everything in memory
---------------------------
127.0.0.1:6379> FLUSHALL
OK
127.0.0.1:6379> GET foo
(nil)
---------------------------

now we want to create an object which stores some key values:
for instance we want to have an object named server. we specify key/value pairs after colon
---------------------------
127.0.0.1:6379> SET server:name myserver
OK
127.0.0.1:6379> GET server:name
"myserver"
---------------------------
or adding port to it:
---------------------------
127.0.0.1:6379> SET server:port 6379
OK
127.0.0.1:6379> GET server:port
"6379"
---------------------------

we can set an expiration with EXPIRE command:
---------------------------
127.0.0.1:6379> SET resource:foo hello
OK
127.0.0.1:6379> EXPIRE resource:foo 120
(integer) 1
---------------------------
the object resource will be removed after 120s.
---------------------------
now let's test to see how much longer until it expires with TTL command?
---------------------------
127.0.0.1:6379> TTL resource:foo
(integer) 18        // it means 18s left
---------------------------
if we come back and execute the command after 120s, we get -2:
---------------------------
127.0.0.1:6379> TTL resource:foo
(integer) -2
---------------------------
-2 means that it's not set
if we initial the resource object again without expiration, and run TTL command, we get -1
---------------------------
127.0.0.1:6379> TTL resource:foo
(integer) -1
---------------------------
-1 means it never expire.

INCRBY increments by specified amount
DECRBY decrements by specified amount

MSET: (Multiple SET)
Sets multiple keys to respective values.
Replaces existing values with new ones:
--------------
MSET key1 "val1" key2 "val2"
--------------

MSETNX:
Sets multiple keys to respective values as long as none of the keys exist.
Will NOT overwrite existing values
Will NOT perform if even a single key already exists.
--------------
MSETNX key1 "val1" key2 "val2"
--------------
if key1 already exists, it won't be overwrite and key2 won't execute likewise

MGET:
Returns values of all specified keys.
Nill is returned if key doesn't hold value
--------------
MGET key1 key2
--------------

APPEND:
if key already exist and is a string, the value will be appended at the end pf the string
if the key does NOT exist. it works as SET
--------------
APPEND mykey "stringtoappend"
--------------

GETRANGE:
Return the substring of a value.
Determined by offsets start and end.
Negative offsets can be used to start from the end of the string
--------------
GETRANGE mykey 0 -1     // it's gonna return entire thing: 0 means very begining and -1 is the last item
GETRANGE mykey 0 3      // returns till the forth character
GETRANGE mykey -1 0     // returns everything from last itme to the first
--------------

Rename:
it Renames a key. Returns error if key doesn't exist. if does exist, it is overwritten.
--------------
RENAME mykey myRenamedKey
--------------

RENAMENX:
it will rename a key to another key-name, but the new key-name shouldn't be existed before.
--------------
127.0.0.1:6379> RENAMENX key2 mykey
(integer) 0     // mykey is already exist.
--------------

GETSET:
this is going to automatically sets key to value and returns the old value.
Returns error when key exists
can be used with INCR for counting with automatic reset.
--------------
GETSET mykey "myval"
--------------
let's say you want a counter but you want to save the last value of that key. so you use GETSET

SETEX:
Set a key to hold a string value and timeout after a given amount of seconds
we already had:
--------------
SET mykey "hello"
EXPIRE mykey 10
--------------
now we have:
--------------
SETEX mykey 10 "hello"
--------------

PSETEX:
is the same as SETEX except it uses milliseconds instead of seconds.
--------------
PSETEX mykey 1000 "hello"

PTTL:
is used to get the remaining time in milliseconds.
--------------

PERSIST:
That's going to remove an existing timeout on a key.
--------------
PERSIST mykey
--------------

SETNX:
this works like SET if the key does not exist.
if the key already exists, it will not change.
--------------
SETNX mykey "hello"
--------------

Exercise1:
--------------------------------------------------------
127.0.0.1:6379> SET mystring "This is my string"
OK
127.0.0.1:6379> GETRANGE mystring 0 -1
"This is my string"
127.0.0.1:6379> GETRANGE mystring 0 5
"This i"
--------------------------------------------------------
127.0.0.1:6379> SETEX key1 20 "Hello"
OK
127.0.0.1:6379> TTL key1
(integer) 15
--------------------------------------------------------
127.0.0.1:6379> PSETEX key1 20000 "hello"
OK
127.0.0.1:6379> PTTL key1
(integer) 13822
--------------------------------------------------------
make a key persistend:
--------------------------------------------------------
127.0.0.1:6379> SETEX key1 20 "Hello"
OK
127.0.0.1:6379> TTL key1
(integer) 16
127.0.0.1:6379> PERSIST key1
(integer) 1
127.0.0.1:6379> TTL key1
(integer) -1
--------------------------------------------------------

SCAN:

scan iterates(repeat) the set of keys in the database.
it returns us only a small amount per call which can be used by user
scan used in production because it performs really well 
and it takes a cursor or position as a parameter.

Cuesor Based Iterator:
from that parameter the server will return an updated cursor with every call of the command.
and this can be used as an argument to the next call to kind of pick up where it left off.
the literation starts when cursor is set to 0,
and terminates when cursor returned from the server is 0.

SCAN Guarantees:
full iteration will retrieve all elements that were present in the collection from the satrt to the end.
Never returns any element that was NOT present in the collection from the start to finsh.
so it's really reliable.

SCAN has COUNT option:
COUNT can be defined in a SCAN command to overwrite the default returned per iteration.
the user can specify the amount of work done at every call
default COUNT is 10
COUNT can be changed from one iteration to the next.
--------------
SCAN COUNT 20
--------------

MATCH option:
this will iterate elements that match a specific pattern.
--------------
SCAN 0 MATCH something
SCAN 0 MATCH k*         // scan everything that starts with k
--------------

SCAN with other Data Types:
SSCAN: used with SETs. Returns list of set members
HSCAN: used with hashes. returns array of elements with a field and value
ZSCAN: used with sorted sets. returns array of elements with associated score.


KEYS pattern:
KEYS will return all keys that match a specific pattern.
KYES should be avoided in production enviroments. because it doesn't just give us 
parts or iteration, It returns everything at once. So it can be a little taxing on system.
--------------
h?llo matches hello,hallo and hxllo                 //  anything with one character
h*llo matches hllo and heeeeello                    //  anything with anylength
h[ae]llo matches hello and hallo, but not hillo     //  just a or e
h[^e]llo matches hallo,hbllo,... but not hello      //  anything but not e
h[a-b]llo matches hallo and hbllo                   //  from a till a

use \ to escape special characters if you want to match them verbatim.
--------------
"?" means that it can be anything but only one character.
"*" can be anything by anylength.

RANDOMKEY:
returns a random key from the database

--------------------------------------------------------
127.0.0.1:6379> MSET key1 "1" key2 "2" key3 "3" key4 "4" key5 "5" key6 "6" 
key7 "7" key8 "8" key9 "9" key10 "10" key11 "11" key12 "12" key13 "13"   
OK
127.0.0.1:6379> SCAN 0      // it will give us the first 10
1) "11"                     // the cursor
2)  1) "key8"
    2) "key13"
    3) "key9"
    4) "key11"
    5) "key6"
    6) "key10"
    7) "key12"
    8) "key3"
    9) "key4"
   10) "key2"
127.0.0.1:6379> SCAN 11     // if we want to get other keys, we  should scan from the cursor
1) "0"
2) 1) "key1"
   2) "key7"
   3) "key5"
--------------------------------------------------------

*** So by "SCAN 0" it returns first 10 keys of the collection.
    and also a cursor which can be used as a parameter for next call, 
    in order to get the other keys which left. and if we ran the command with 
    the cursor, and it gave us 0, it means it did the whole thing.
    Whanever we use different kind of SCAN commands, it also returns a cursor

--------------------------------------------------------
127.0.0.1:6379> SCAN 0 COUNT 3      // we just want the 3 first keys
1) "4"                              // it also give us a cursor for the rest of the keys
2) 1) "key8"
   2) "key13"
   3) "key9"
127.0.0.1:6379> SCAN 4              // by scanning the cursor, we get the rest of the keys.
1) "15"
2)  1) "key11"
    2) "key6"
    3) "key10"
    4) "key12"
    5) "key3"
    6) "key4"
    7) "key2"
    8) "key1"
    9) "key7"
   10) "key5"
--------------------------------------------------------
127.0.0.1:6379> SCAN 0 MATCH key1*      // give some of the keys
1) "11"
2) 1) "key13"
   2) "key11"
   3) "key10"
   4) "key12"
127.0.0.1:6379> SCAN 11 MATCH key1*     // give the rest of the keys
1) "0"
2) 1) "key1"    
--------------------------------------------------------
127.0.0.1:6379> KEYS *                  // it gives everything at once, without cursor. Not good for production
 1) "key8"
 2) "key2"
 3) "key11"
 4) "key4"
 5) "key5"
 6) "key13"
 7) "key9"
 8) "key12"
 9) "key3"
10) "key6"
11) "key1"
12) "key7"
13) "key10"
--------------------------------------------------------
127.0.0.1:6379> Keys key1*              // give us anything that matches the pattern at once.
1) "key11"
2) "key13"
3) "key12"
4) "key1"
5) "key10"
------------------------------------------------------
127.0.0.1:6379> RANDOMKEY
"key2"
127.0.0.1:6379> RANDOMKEY
"key11"
127.0.0.1:6379> RANDOMKEY
"key2"
127.0.0.1:6379> RANDOMKEY
"key5"
--------------------------------------------------------

Commands for configuration and clients:

CONFIG GET:
used to read the configuration parameters of a running Redis server.
all config params are allowed since version 2.6
CONFIG GET takes a single argument which can be a specific option such as in this example:
---------------
CONFIG GET port - Gets the port configuration value.
CONFIG GET * - List all supported config params.
CONFIG GET *max-*-entires* - hash-max-zipmap-entires, list-max-ziplist-entires,etc.
---------------

CONFIG SET:
used to reconfigure server at runtime without having to do a restart
---------------
CONFIG SET configuration "newvalue"     // syntax: CONFIG SET [option] "value"
---------------

INFO:
INFO command returns information and statistics about a server.
Optional parameters can be used to select specific sections of information.
optional parameters: 
server | clients | memory | persistence | stats | replication | CPU
commandstats | cluster | keyspace | all | default
---------------
INFO
INFO server     // it will only returns parameters that have to do with the server
---------------

CONFIG RESETSTAT:
this is used to reset statistics reported using the info command
it's not gonna reset everything, these are the certain parameters that can be reset:

---------------
Keyspace hits
Keyspace misses
Number of commands processed
Number of connections received
Number of expired keys
Number of rejected connections
---------------
the above things can be reset using below command:
---------------
CONFIG RESETSTAT
---------------

COMMAND commands:
returns details about all redis commands

Each top level result contains 6 sub-results...

Command name
Command arity - Number of params
Nested array reply of command flags
position of first key in arg list
position of last key in arg list
step count for location or repeating keys


COMMAND INFO:
returns details for a specific command.
Same as COMMAND but focus on specific command
--------
COMMAND INFO GET
--------

COMMAND COUNT:
which will return the number of available commands on the server
--------
COMMAND COUNT
--------

CLIENT LIST:
returns info and statistics on the clients connected to a server
when we say clients, that could be a number of things.
if we connect through a terminal inside of our Linux system, that's considered a client
if we open another terminal, that's another client.
by CLIENT LIST we can get:

ID, Name, Flags, Age, Address/Port, Last command, Idle time


CLIENT SETNAME:
this will allow us to assign a name to a current client connection.
this will be displayed in output of CLIENT LIST.

CLIENT SETNAME clientname

CLIENT GETNAME:
Returns the name of the current client connection
null reply if no names is set.

CLIENT KILL:
Close up a given connection
Can use addrress/port or ID
to be able to kill a client.
------------
CLIENT KILL 127.0.0.1:portnum

CLIENT KILL id
-------------------------------------------------------
127.0.0.1:6379> CONFIG GET port
1) "port"
2) "6379"
-------------------------------------------------------
127.0.0.1:6379> CONFIG GET *
  1) "dbfilename"
  2) "dump.rdb"
  3) "requirepass"
  4) ""
  5) "masterauth"
  6) ""
  7) "cluster-announce-ip"
  8) ""
  9) "unixsocket"
 10) ""
 11) "logfile"
 12) ""
 13) "pidfile"
 14) ""
 ...
 ...
 ...
210) ""
211) "notify-keyspace-events"
212) ""
213) "bind"
214) ""
-------------------------------------------------------
127.0.0.1:6379> CONFIG GET *max-*-entries*
1) "hash-max-ziplist-entries"
2) "512"
3) "set-max-intset-entries"
4) "512"
5) "zset-max-ziplist-entries"
6) "128"
-------------------------------------------------------
127.0.0.1:6379> CONFIG GET lua-time-limit
1) "lua-time-limit"
2) "5000"
127.0.0.1:6379> CONFIG SET lua-time-limit 6000
OK
127.0.0.1:6379> CONFIG GET lua-time-limit
1) "lua-time-limit"
2) "6000"
-------------------------------------------------------
127.0.0.1:6379> INFO
# Server
redis_version:5.0.7
redis_git_sha1:00000000
redis_git_dirty:0
redis_build_id:295beb9462eefd91
redis_mode:standalone
os:Darwin 19.3.0 x86_64
arch_bits:64
multiplexing_api:kqueue
atomicvar_api:atomic-builtin
gcc_version:4.2.1
process_id:49741
run_id:ff5e654c574a7a947be207aa75567c072685e75f
tcp_port:6379
uptime_in_seconds:26790
uptime_in_days:0
hz:10
configured_hz:10
lru_clock:5855057
executable:/Users/armanfeili/redis-server
config_file:

# Clients
connected_clients:2
client_recent_max_input_buffer:2
client_recent_max_output_buffer:0
blocked_clients:0

# Memory
used_memory:1070432
used_memory_human:1.02M
used_memory_rss:3887104
used_memory_rss_human:3.71M
allocator_active:3849216
allocator_resident:3849216
total_system_memory:17179869184
total_system_memory_human:16.00G
used_memory_lua:37888
used_memory_lua_human:37.00K
lazyfree_pending_objects:0

# Persistence
loading:0
rdb_changes_since_last_save:0
rdb_bgsave_in_progress:0
rdb_last_save_time:1582910373
rdb_last_bgsave_status:ok
rdb_last_bgsave_time_sec:0

# Stats
total_connections_received:3
total_commands_processed:78
instantaneous_ops_per_sec:0
total_net_input_bytes:3116
total_net_output_bytes:33033
instantaneous_input_kbps:0.00
instantaneous_output_kbps:0.00
rejected_connections:0
sync_full:0

# Replication
role:master
connected_slaves:0
master_replid:3670358be577a9d5952b931087ffdada1e3db4e8
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:0

# CPU
used_cpu_sys:5.857105
used_cpu_user:4.129684
used_cpu_sys_children:0.004637
used_cpu_user_children:0.000912

# Cluster
cluster_enabled:0

# Keyspace
db0:keys=13,expires=0,avg_ttl=0
-------------------------------------------------------
127.0.0.1:6379> INFO memory
# Memory
used_memory:1069520
used_memory_human:1.02M
used_memory_rss:4743168
used_memory_rss_human:4.52M
used_memory_peak:1070432
used_memory_peak_human:1.02M
used_memory_peak_perc:99.91%
used_memory_overhead:1055088
used_memory_startup:987824
used_memory_dataset:14432
used_memory_dataset_perc:17.67%
allocator_allocated:1024368
allocator_active:4705280
allocator_resident:4705280
total_system_memory:17179869184
total_system_memory_human:16.00G
used_memory_lua:37888
-------------------------------------------------------
127.0.0.1:6379> COMMAND INFO
...
... everything
127.0.0.1:6379> COMMAND INFO readonly
1) 1) "readonly"
   2) (integer) 1
   3) 1) fast
   4) (integer) 0
   5) (integer) 0
   6) (integer) 0
-------------------------------------------------------
we have 2 clients now:

127.0.0.1:6379>  CLIENT LIST
id=4 addr=127.0.0.1:51411 fd=9 name= age=27196 idle=78 flags=O db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=monitor
id=6 addr=127.0.0.1:62865 fd=8 name= age=212 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=26 qbuf-free=32742 obl=0 oll=0 omem=0 events=r cmd=client
-------------------------------------------------------
127.0.0.1:6379> CLIENT SETNAME bar
OK
-------------------------------------------------------
127.0.0.1:6379> CLIENT GETNAME
"bar"
-------------------------------------------------------
127.0.0.1:6379> CLIENT KILL 127.0.0.1:62865
OK
127.0.0.1:6379> CLIENT LIST
id=4 addr=127.0.0.1:51411 fd=9 name= age=27405 idle=287 flags=O db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=monitor
id=7 addr=127.0.0.1:63740 fd=8 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=26 qbuf-free=32742 obl=0 oll=0 omem=0 events=r cmd=client

it killed the previous client with port 62865, and added new client with new port.
-------------------------------------------------------

Data Types

Lists:
lists are basically group of strings.like one dimentional array of strings
they are sorted by order of insertion. what element you put in last, will show up first.
Elements can be pushed on the head or tail of the list
lists are often used as producer/consumer queries.

inserting items into the list on one hand and then poping them off of the other
and items can be removed automatically.
this works really well for things like real time messaging or event cues and notification systems

inserting Elements:
LPUSH : Inserts a new element on the head (left)
RPUSH : Inserts a new element on the tail (right)

if you don't have list and try to push to it, it will create that list.
the key will be removed from the keyspace if a list operation will empty the list
----------
LPUSH mylist a
"a"
LPUSH mylist b
"b","a"
LPUSH mylist c
"b","a","c"
----------

LRANGE:
Returns specified elements of the list.
the offsets are zero-based indexes.
offsets can be negative indicationg offsets starting from the end of the list
--------------------------------------------
127.0.0.1:6379> LPUSH friends Eric Shawn Jose
(integer) 3
127.0.0.1:6379> LRANGE friends 0 -1
1) "Jose"
2) "Shawn"
3) "Eric"
--------------------------------------------

LLEN:
returns the length of the list
----------
LLEN friends
3
----------

LPOP:
is going to remove and return the first element of a list.

RPOP:
is going to remove and return the last element of a list.

LINSERT:
insert an element before or after a specific element using BEFORE or AFTER command.
--------------------------------------------
127.0.0.1:6379> LINSERT friends BEFORE "Shawn" "Kevin"
(integer) 4
127.0.0.1:6379> LRANGE friends 0 -1
1) "Jose"
2) "Kevin"
3) "Shawn"
4) "Eric"
--------------------------------------------


Sets:

Sets are unordered collection of strings
we can add/remove and test for existence of its members.
they don't allow repeating members. so no matter how many times you try to add a member, it's only 
going to show up in the set once.
They support server side commands to compute sets starting from existing sets.
this will allow us to perform unions, intersections and differences in very short amount of time.

commands with Sets:

SADD:
it will add given value to a set. and values that already exist will be ignored.
so we don't have to test it to see if sth is there or not.
we don't have to worry about having duplicates

SREM:
it removes values from a set.

SADD carmakes "Toyota"  // Adds Toyota to carmakes set
SREM carmakes "Honda"   // Removes Honda from carmakes set.

SISMEMBER
Test if the given value is in the set
it returns 1 if the value is there and 0 if it is not

SISMEMBER carmakes "Toyota"   // check if Toyota is in the particular set.
Returns a list of all of the members of a set. just like LRANGE for list

SMEMBERS:
returns a list of all of the members of a set.

SMEMBERS carmakes

SCARD:
will return the count of members / elements in a set
so it returns 0 if the key does not exist.

SCARD carmakes

SMOVE:
moves memebrs from one set to another.

SMOVE people users "John Doe"    // move John Doe from set people to the set users.

SUNION:
will combine two or more sets and returns a list of members

SUNION carmakes truckmakes

SDIFF:
Returns the members of the set resulting from the difference between the first and all successive sets.
keys that do not exist are considered empty sets.

SDIFF key1 key2      // key1 is one set, key2 is another set


SRANDMEMBER:
returns a random member of a set
optional parameter to return a specified count

SRANDMEMBER carmakes       // return one random member
SRANDMEMBER carmakes 3     // return 3 random memebrs

SPOP:
removes and returns a random member from a specified set.
Like SRANDMEMBER, a second parameter is allowed to specify a count of members.

SPOP carmakes        // remove and return a random member
SPOP carmakes 3      // remove and return 3 random memebrs
------------------------------------------------------
127.0.0.1:6379> SADD carmakes "Toyota"                // add one member to set
(integer) 1
127.0.0.1:6379> SADD carmakes "Ford" "Chevy" "Honda"  // add 3 members to set
(integer) 3
127.0.0.1:6379> SISMEMBER carmakes "Honda"            // check if Honda exists
(integer) 1
127.0.0.1:6379> SMEMBERS carmakes                     // see all set members.
1) "Honda"
2) "Chevy"
3) "Ford"
4) "Toyota"
127.0.0.1:6379> SADD carmakes "Honda"                 // add Honda again but we get 0, because it already exists.
(integer) 0
127.0.0.1:6379>  SCARD carmakes                       // count the set members
(integer) 4
127.0.0.1:6379> SADD mycars "Acura"                   // add another set with one member
(integer) 1
127.0.0.1:6379> SMOVE carmakes mycars "Toyota"        // move Toyota from carmakes to mycars
(integer) 1
127.0.0.1:6379> SMEMBERS mycars                       // see that Toyota is added to mycars
1) "Toyota"
2) "Acura"
127.0.0.1:6379> SMEMBERS carmakes                     // see that Toyota doesn't exist in carmakes
1) "Chevy"
2) "Ford"
3) "Honda"
127.0.0.1:6379>  SUNION carmakes mycars               // It combine the given sets and return all members
1) "Honda"                                            // but still carmakes and mycars do exist.
2) "Ford"
3) "Chevy"
4) "Toyota"
5) "Acura"
127.0.0.1:6379> SRANDMEMBER carmakes                  // gives random member
"Honda"
127.0.0.1:6379> SRANDMEMBER carmakes                  // gives random member
"Ford"
127.0.0.1:6379> SRANDMEMBER carmakes 2                // gives 2 random members
1) "Honda"
2) "Chevy"
127.0.0.1:6379> SPOP carmakes                         // ramdomally delet a member from set and return
"Chevy"
127.0.0.1:6379> SMEMBERS carmakes                     // there is no Chevy
1) "Ford"
2) "Honda"
------------------------------------------------------

SORTED SETS:

Regular sets can pose problems for some projects because they are completely unsorted.
to fix this Redis also uses sorted sets.
Every member is associated with a "score"
and this allows us to sort and access data very quickly
an Element can only apear once because it is still a set
the only difference is that it now has a score.
Now an element is still defined by its contents or its value
so if you have a member with name of John and score of 1,
an then if you have another member with name of John and score of 2, 
you won't have 2 Johns, you will have One John and the score will be overwritten from 1 to 2.

Score properties:
Score is required. they must be a float/number
Score of 500 = 500.0 => so it's formatted as a float.
Scores are not unique, but values are unique.

ZADD:
adds given value to a sorted set

ZREM:
removes values from a sorted set.

ZADD people 1960 "John Doe"      // the number 1960 is a score
ZREM people "John Doe"
  
Score can be any number, it can be a year, or 123, it doesn't have significance but it can.

ZRANGE:
Works like LRANGE for Lists, it Fetches value within a specified range.
it is ordred lowest to highest by score.

ZREVRANGE:
Same as ZRANGE except orderd highest to lowest.
---------
ZRANGE people 0 -1
ZRANGE people 2 4
ZREVRANGE people 0 -1
---------

ZRANGEBYSCORE:
It works like ZRANGE but by score instead of contents.
---------
ZRANGEBYSCORE people 1950 1990      // gets people with a score between 1950 and 1990
---------

ZRANK:
Returns the rank of a member with scores ordered from high to low rank is 0-based.

ZREVRANK:
Gets the rank of a member in the reverse order.
---------
ZRANK people "John Doe"
ZREVRANK people "John Doe"
---------

ZCARD:
returns the number of members in the sorted set
---
ZCARD people
---

ZCOUNT:
Returns the number of elements in the sorted set at key with a score between min and max
---
ZCOUNT people(1,3)      // returns the count of items

ZINCRBY:
increments the score of member in the sorted set
if member does not exist, it will be added with increment as its score.
the score value should be the string representation of a numeric value, and
accepts double percision floating point numbers. It is possible to
provide a negative value to decrement the score.
---
ZINCRBY people 1 "John Doe"

ZSCORE:
will return the score of a member,and if member does not exist, nil will be returned.
---
ZSCORE people "John Doe"
--------------------------------------------------------
127.0.0.1:6379> ZADD people 1970 "John Doe"           // add a member to the ordered-list of people with the score of 1970
(integer) 1
127.0.0.1:6379> ZADD people 1985 "Sam Smith"          // add another member
(integer) 1
127.0.0.1:6379> ZADD people 1990 "Jen Williams"       // add another member
(integer) 1
127.0.0.1:6379> ZRANGE people 0 -1                    // return all members
1) "John Doe"
2) "Sam Smith"
3) "Jen Williams"
127.0.0.1:6379> ZRANGEBYSCORE people 1970 1986        // return all members between two scores (years)
1) "John Doe"
2) "Sam Smith"
127.0.0.1:6379> ZRANK people "Sam Smith"              // return the rank of any element, rank is 0 based
(integer) 1
127.0.0.1:6379> ZRANK people "John Doe"               // return rank
(integer) 0
127.0.0.1:6379> ZRANK people "Jen Williams"           // return rank
(integer) 2
127.0.0.1:6379> ZREVRANK people "Jen Williams"        // it returns 0 because we sorted from opposite direction 
(integer) 0
127.0.0.1:6379> ZCARD people                          // returns the number of members
(integer) 3
127.0.0.1:6379> ZINCRBY people 1 "John Doe"           // increments the score by 1
"1971"
127.0.0.1:6379> ZINCRBY people 5 "John Doe"           // increments the score by 5
"1976"
127.0.0.1:6379> ZSCORE people "John Doe"              // get score
"1976"
-----------------------------------------------------------

Hash:

Hash is maps between string fields and string values.
this is perfect for representiong objects.
Hash Redis works pretty similar to JSON object.
hashes are very compact. they can fit up to sth like 4 billion field value pairs in a hash.
so it's good to hold a lot of data in a small space.

HSET:
it sets a field in a hash
it also create the hash if there are no fields yet and if it's not created.
if a key does not exist, a new key holding a hash is created.
if a field exists in the hash, it will be overwritten.

it returns 1 if the field is a new field and the value is set in the hash.
it's gonna return 0 if the field already exists and the value is updated.
---
HSET user1 name "John"     // here we set a hash named user1, and we set a "name" property with value of "John"
---

HMSET:
it sets multiple fields to their respective values.
and it overwrite any existing files in the hash.
---
HMSET user2 name "Jill" email "jill@gmail.com" age "25"
---

HGET:
it gets a value associated with a field in a hash,
it returns value or nill if the field is not present.
---
HGET user1 name
---

HMGET:
returns value associated with multiple fields in a hash.
for every field that does not exist, a nill value is returned.
---
MGET user1 name age     // here we're gonna grab the name and age fields from the user1 hash.
---

HGETALL:
It gets all fields and values in a hash
ir returns every field name followed by its value.
---
HGETALL users           // get all the fields and values of the users hash.
---

HDEL:
will remove the specified field from the hash.
specified field that do not exist are ignored.
it's gonna return the number of files that were removed from the hash.
---
HDEL user2 age          // we delete age field from user2 hash.
---

HEXISTS:
it checks for an existing field in a hash.
it returns 1 if the hash contains the field and 0 if it does not.
---
HEXISTS name user3
---

HINCRBY:
it increments the number sorted in the hash
if key does not exist, a new key holding a hash is created
if field does not exist the value is set to 0 before operation is performed.
---
HINCRBY user3 age 1     // we take age field from user3 hash, and we increment it by 1
---

HKEYS:
returns all the field names in the hash and not the actual value. just the field name.
---
HKEYS user1
---

HVALS:
Returns all values in a hash
---
HVALS user1
---

HLEN:
Returns the number of t=fields contained in the hash
Returns 0 if key does not exist
---
HLEN user 1
---

HSTRLEN:
Returns the string length of the value associated with the field in the hash
if the key or field do not exist, 0 is returned
---
HSTRLEN userr1 name

----------------------------------------------------------
127.0.0.1:6379> HSET user:John name "John Doe"        // create a hash named "user:John" with field of name which filled by "John Doe"
(integer) 1
127.0.0.1:6379> HGET user:John name                   // Get the name field
"John Doe"
127.0.0.1:6379> HMSET user:kate name "Kate Smith" email "kate@gamil.com" age "30"      // set multiple fields
OK
127.0.0.1:6379> HGET user:kate name                   // get one field.
"Kate Smith"
127.0.0.1:6379> HGET user:kate email
"kate@gamil.com"
127.0.0.1:6379> HGET user:kate age
"30"
127.0.0.1:6379> HMGET user:kate name age              // get multiple fields of a hash
1) "Kate Smith"
2) "30"
127.0.0.1:6379> HGETALL user:kate                     // it gives us all field names as well as all values
1) "name"
2) "Kate Smith"
3) "email"
4) "kate@gamil.com"
5) "age"
6) "30"
127.0.0.1:6379> HKEYS user:kate                       // give us just keys of the hash
1) "name"
2) "email"
3) "age"
127.0.0.1:6379> HVALS user:kate                       // give us just values of the hash
1) "Kate Smith"
2) "kate@gamil.com"
3) "30"
127.0.0.1:6379> HINCRBY user:kate age 1               // increment age of this user hash
(integer) 31
127.0.0.1:6379> HDEL user:kate age                    // delete a field of a hash
(integer) 1
127.0.0.1:6379> HGETALL user:kate                     // see that age doesn't exist anymore.
1) "name"
2) "Kate Smith"
3) "email"
4) "kate@gamil.com"
127.0.0.1:6379> HLEN user:kate                        // the length of hash (Count of fields of the hash)
(integer) 2
----------------------------------------------------------

Data Persistence Features:

Datasets are all sorted in memory so it makes data storage and data retrieveal very fast.
we also have the option to save it to a disk or persist the data.
Redis forking: this means creating a child processes which are the exact copy of the database (parent)
this allows us to make changes and not affect the original data set.
it uses sth name Copy-On-Write Snapshot and this will allow the system to 
create or fork the backup without making us wait for it.

Persistence Process:
1: client sends write command to database (client memory)
   at this point the data is only in the client's memory.
2: then the database receives the write (server memory)
   so it will move and gets stored into the server memory
3: next, the database will call before the system call that write data on to the list.
   at this point, it's stored in the kernel buffer.
   if we stop the server and started again the data would still be there.
   However if there was a power failure or sth like that, the data would be lost.
   *** this would be th final stage by default if we don't use any kind of data persistence
4: if we have persistence setup, the operating system (OS) transfers the right buffer 
   to the disk controller. (Disk cache)
5: finally the disk controller writes the data to the physical disk.(physical media)


Pools:
pool in redis means where we can have multiple Redis servers running on the same machine 
using different ports. so we have:
1. more efficient memory storage - when snapshotting only databases from one pool
   are copied into the memory at a time.
2. More CPUs can be used. so each instance only uses one CPU but different pools can 
   use different cores. which make it a liitle faster.
3. we can also have better fine-tuning as each pool can be configured and restarted on
   its own.


Replication (the action of copying or reproducing something):

It uses a simple master-slave replication that allows slave Redis servers
to be copies of Master servers.
- Redis uses Asynchronous replication which is a store and forward approach to data backup.
- a single master can have multiple slaves in replication
- slaves can accept connections from other slaves. and this is done in a graph like structure
- Replication is nonblocking on both the master and the slave side
  so the Master will continue to serve queries when slaves perform synchronization.
- Replication can be used to scale or can also be used for data redundancy (the state of being not or no longer needed or useful)
  and you can always set slaves to be in read only mode.

Replication Process:
1. Master starts saving in the background and starts buffering all new commands that will modify the dataset.
2. After background saving, the master transfers the database file to the slave
3. and then the slave, saves the files to the disk and loads it into memory
4. and finally, the master will send all the buffered commands to the slave.

Persistence options:
1. RDB - it offers Point-in-time snapshot
2. AOF - which is append only file - this uses write operation loging
3. Disabled - not use persistence at all and keep  evrything in memory and uses for cache
4. use Both RDB & AOF - the most effective type of persistence.

RDB - Redis Database file:
simplest type of persistance mode.
RDB is enabled by default and 
it's essentially just a single file with point and time representation of the data
RDB works by taking a snapshot of the data set.

it's easy to use and it's enabled by default so we don't have to enable it
it's very compact and perfect for backup and recovery.
it can also maximize Redis performance, because all Redis has to do is fork a child process to 
handle the rest, asynchronously.
RDB also allows for faster restarts with big data sets compared to AOF
which is the other persistent method that Redis uses.

Snapshotting:
a Snapshot is basically an exact copy of a system or a database that
can be used to restore it back to a specific state and time.
Snapshot can include structure, data and configuration.
RDB snapshot conditions can be completely controlled by the user.
so the user can set up how often and in what situation these snapshots will occur.
they can be modified at runtime without restarting the server.
snapshots are produced as .rdb files.
we can manually take snapshot with couple commands like:
SAVE command and BGSAVE command.

SAVE:
will perform a synchronous save of the dataset producing a point-in-time snapshot of all
data inside of the Redis instance in the form of an .rdb file
it should not be called in production enviroments as it will block all other clients, Instead use BGSAVE
---
SAVE 60 100
---
above we said, we want to take a snapshot and save all the data set to the disk
every 60 seconds if ther's at least a hundred keys changed.

with BGSAVE, it saves the database in the background so that the parent can continue to serve
clients and it doesn't block ports or other clients.
---
BGSAVE
---

RDB disadvantages:
we're limited to save points
so let's say the data set is saved every 15 minutes, that means if there is some kind of
failure or disaster you can lose up 15 minutes of rights.
so it will not good if you want to really minimize the chance of data loss,
if Redis for some reasons stops working.
It needs to fork often which is that means to create child processes and this can be time consuming.
and can wear on CPU performance.


AOF: Append Only File 
this is considered to be the main persistence option
it's much more reliable than RDB snapshotting
and how it works is basically every operation that modifies the data set in memory
gets logged. so it creates an instruction set for your database
the logs are in the same format that is used by clients to communicate with Redis
AOF can actually be piped to another instance.
the entire data set can be reconstructed using these logs or instructions 
and only the operations and commands that actually modify the data structure will be logged.
so not evrything will be logged, only what's needed to build it back up to
how it was before.

when we're using AOF, we're dealing with a file that is always growing.(the log file)
so what if it gets too big? what happens is Redis will rewrite the database from scratch
into a temporary file.
the rewrite isn't performed by reading the old one, but by directly accessing the data in memory.
so in that way Redis can create the shortest AOF file, that it can generate and
it won't need disk access while writing the new one.
And once that's written the temporary file will be sinked up to disk using "fsync".

fsync Policies:
there are a few options as far as fsync policies.

1.No fsync -  we can choose no fsync at all, and leaving it up to operating system.
and this would be around every 30 seconds or so

2.fsync every second (default): even though it's every second, we still get grate right
performance because fsync is performed to run on a background thread.

3.fsync at every query: this would generally slow down performance a liitle bit.

so in most cases we want to leave it as dfault fsync at every second.

AOF advantages:
AOF tends to be much more durable than RDB, mostly due to the three different fsync policies
that we can create. it's only possible to lose one second worth of rights in the event of some type of failure.
AOF being a single file really eliminates any corruption issues
so even if the log for some reason ends with a half written command, Redis has a check tool that can fix that pretty easily.
so corruption is not sth that we need to worry about.
AOF automatically rewrites on the background if it gets too big.
if the file needs to be rewritten, the old one, it is still going to be tended to while the new ones being written.
so no data loss or any problems there.
it's also pretty easy to undrestand the log or the instructions

AOF disadvantages:
it takes longer to load in memory when a server restarts.
the equivalent AOF file is bigger that ne RDB file.
AOF can be slower than RDB depending on the fsync policy.
AOF has more possible bugs.

the default RDB database is located in
"usr/local/var/db/redis/dump.rdb"
we can locate all rdb files with:
$ sudo locate *rdb
----------------------------------------------------------------
127.0.0.1:6379> SET user:john "John Doe"
OK
127.0.0.1:6379> SET user:mary "Mary Smith"
OK
127.0.0.1:6379> SET user:steve "Steve Williams"
OK
127.0.0.1:6379> SET user:derek "Derek Jackson"
OK
127.0.0.1:6379> SET user:george "George Samson"
OK
127.0.0.1:6379> SAVE          // by writing SAVE, RDB file will automatically be updated
OK                            // because RDB is default persistance
----------------------------------------------------------------

but we should copy this file somewhere else.

now we want to setup a program that automatically does the copy
and whatever time we want. the program is rdiff-backup
---
% brew install rdiff-backup 
---
then
---
rdiff-backup --preserve-numerical-ids usr/local/var/db/redis ~/code/Redis-backup/rdiff-backup 
---

we can set up a cronjob using crontab

this was RDB for makeing a backup. but if you want to save data on disk, you should use AOF

in order to make AOF start, we use BGREWRITEAOF:
----------------------------------------------------------------
% redis-cli
127.0.0.1:6379> BGREWRITEAOF
Background append only file rewriting started
----------------------------------------------------------------
127.0.0.1:6379> INFO
# Server
redis_version:5.0.7
redis_git_sha1:00000000
redis_git_dirty:0
redis_build_id:295beb9462eefd91

# Clients
connected_clients:1
client_recent_max_input_buffer:2
client_recent_max_output_buffer:0
blocked_clients:0

# Memory
used_memory:1073168
used_memory_human:1.02M
used_memory_rss:3301376
used_memory_rss_human:3.15M

# Persistence
loading:0
rdb_changes_since_last_save:0
rdb_bgsave_in_progress:0
rdb_last_save_time:1583053497
rdb_last_bgsave_status:ok
rdb_last_bgsave_time_sec:0
rdb_current_bgsave_time_sec:-1
rdb_last_cow_size:0
aof_enabled:0
aof_rewrite_in_progress:0
aof_rewrite_scheduled:0
aof_last_rewrite_time_sec:0
aof_current_rewrite_time_sec:-1
aof_last_bgrewrite_status:ok
aof_last_write_status:ok
aof_last_cow_size:0

# Stats
total_connections_received:2
total_commands_processed:10
instantaneous_ops_per_sec:0
total_net_input_bytes:342

# Replication
role:master
connected_slaves:0

# CPU
used_cpu_sys:1.802703
used_cpu_user:1.229219
used_cpu_sys_children:0.003429
used_cpu_user_children:0.001066

# Cluster
cluster_enabled:0

# Keyspace
db0:keys=5,expires=0,avg_ttl=0
----------------------------------------------------------------

in Persistence, we can see all rdb and aof persistence method
look at the "aof_rewrite_in_progress:0"
if that's 0, it means that it already ran.
if it's 1, it means rewrite is in progress.

we also should change the value in config file
so we should stop redis service for that.
---
$ nano /usr/local/etc/redis.conf
---
and where it is written "appendonly no"
we change it to "appendonly yes"

and then start the service again.

and if we see the redis folder again, we can see appendonly.aof file there:
---
% ls /usr/local/var/db/redis/
appendonly.aof	dump.rdb
---
and we can use the rdiff command to copy it into our home folder.
---
% rdiff-backup --preserve-numerical-ids usr/local/var/db/redis ~/code/Redis-backup/rdiff-backup
---
it brings the appendonly.aof file to the folder we want
if we open it it has all the changes we had in our database, plus all keys and values:
-------------------------------------------
REDIS0009˙	redis-ver5.0.7˙
redis-bits¿@˙ctime¬µé[^˙used-mem¬P ˙aof-preamble¿˛ ˚  user:george
George Samson 	user:johnJohn Doe 
user:derek
Derek Jackson 	user:mary
Mary Smith 
user:steveSteve Williamsˇsk∑™Lå…
-------------------------------------------
if we write:
-------------------------------------------
armanfeili@Armans-MacBook-Pro / % redis-cli
127.0.0.1:6379> SET user:tom "Tom Doe"
OK
127.0.0.1:6379> exit
armanfeili@Armans-MacBook-Pro / % rdiff-backup --preserve-numerical-ids usr/local/var/db/redis ~/code/Redis-backup/rdiff-backup
-------------------------------------------
we can see that "Tom Doe" is added to the disk by AOF.
-------------------------------------------

Now we're gonna build an application.
we use node_redis client as it's Recommended client for node.	

all commands in redis are available in nodejs:
------------------------------------------------------------------------
var express = require("express");
var redis = require("redis");

// Create client
var client = redis.createClient();
// connect the client - by writing this, we can run $redis-cli
client.on("connect", () => {
  console.log("Redis server connected");
});
------------------------------------------------------------------------
for example assume that we have a task List in redis and wants to get data by LRANGE
------------------------------------------------------------------------
app.get("/", (req, res) => {
  var title = "Task List";

  // client has all redis commands
  // lrange takes the list name, the pointers, and a callback for what to do next.
  // the callback takes the err and reply. reply means the result for LRANGE
  client.lrange("tasks", 0, -1, (err, reply) => {
    // we bring render into this method to see results in client.
    res.render("index", {
      title,
      tasks: reply
    });
  });
});
------------------------------------------------------------------------
we can see lists as an array.
------------------------------------------------------------------------

Publish–subscribe pattern:"PubSub"

In software architecture, publish–subscribe is a messaging pattern where senders of messages, 
called publishers, do not program the messages to be sent directly to specific receivers, 
called subscribers, but instead categorize published messages into classes without knowledge 
of which subscribers, if any, there may be. Similarly, subscribers express interest in one 
or more classes and only receive messages that are of interest, without knowledge of which 
publishers, if any, there are.

Publish–subscribe is a sibling of the message queue paradigm, and is typically one part of a 
larger message-oriented middleware system.
This pattern provides greater network scalability and a more dynamic network topology, with a 
resulting decreased flexibility to modify the publisher and the structure of the published data.

Message filtering:
In the publish-subscribe model, subscribers typically receive only a subset of the total messages 
published. The process of selecting messages for reception and processing is called filtering. 
There are two common forms of filtering: topic-based and content-based.

In a topic-based system, messages are published to "topics" or named logical channels. 
Subscribers in a topic-based system will receive all messages published to the topics to which 
they subscribe. The publisher is responsible for defining the topics to which subscribers can subscribe.

In a content-based system, messages are only delivered to a subscriber if the attributes or 
content of those messages matches constraints defined by the subscriber. The subscriber is 
responsible for classifying the messages.

Some systems support a hybrid of the two; publishers post messages to a topic while subscribers 
register content-based subscriptions to one or more topics.

https://www.npmjs.com/package/ioredis#error-handling

https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern

https://redis.io/topics/pubsub


Pattern: Navigation session
Imagine you have a web service and you are interested in the latest N pages recently visited by your 
users, such that each adjacent page view was not performed more than 60 seconds after the previous. 
Conceptually you may consider this set of page views as a Navigation session of your user, that may 
contain interesting information about what kind of products he or she is looking for currently, so 
that you can recommend related products.

You can easily model this pattern in Redis using the following strategy: every time the user does a 
page view you call the following commands:

MULTI
RPUSH pagewviews.user:<userid> http://.....
EXPIRE pagewviews.user:<userid> 60
EXEC

If the user will be idle more than 60 seconds, the key will be deleted and only subsequent page views 
that have less than 60 seconds of difference will be recorded.

This pattern is easily modified to use counters using INCR instead of lists using RPUSH.

-----------------------------------------------------------------
Redis Cluster Specification
Redis Cluster implements all the single key commands available in the non-distributed version of Redis.
Redis Cluster does not support multiple databases like the stand alone version of Redis. There is just 
database 0 and the SELECT command is not allowed.

https://redis.io/topics/cluster-tutorial

Every Redis Cluster node requires two TCP connections open. The normal Redis TCP port used to serve 
clients, for example 6379, plus the port obtained by adding 10000 to the data port, so 16379 in 
the example.

Clients should never try to communicate with the cluster bus port, but always with the normal Redis 
command port, however make sure you open both ports in your firewall, otherwise Redis cluster nodes 
will be not able to communicate.

The command port and cluster bus port offset is fixed and is always 10000.

Note that for a Redis Cluster to work properly you need, for each node:

The normal client communication port (usually 6379) used to communicate with clients to be open to 
all the clients that need to reach the cluster, plus all the other cluster nodes (that use the 
client port for keys migrations).
The cluster bus port (the client port + 10000) must be reachable from all the other cluster nodes.
If you don't open both TCP ports, your cluster will not work as expected.

Redis Cluster does not use consistent hashing, but a different form of sharding where every key is 
conceptually part of what we call an hash slot.

There are 16384 hash slots in Redis Cluster, and to compute what is the hash slot of a given key, 
we simply take the CRC16 of the key modulo 16384.

Every node in a Redis Cluster is responsible for a subset of the hash slots, so for example you 
may have a cluster with 3 nodes, where:

Node A contains hash slots from 0 to 5500.
Node B contains hash slots from 5501 to 11000.
Node C contains hash slots from 11001 to 16383.
This allows to add and remove nodes in the cluster easily. For example if I want to add a new 
node D, I need to move some hash slot from nodes A, B, C to D. Similarly if I want to remove 
node A from the cluster I can just move the hash slots served by A to B and C. When the node A 
will be empty I can remove it from the cluster completely.

-----------------------------------------------------------------------
SCAN:

var stream = redis.scanStream({
   // only returns keys following the pattern of `user:*`
   match: "user:*",
   // returns approximately 100 elements per call
   count: 100
});
stream.on("data", function(resultKeys) {
   // `resultKeys` is an array of strings representing key names.
   // Note that resultKeys may contain 0 keys, and that it will sometimes
   // contain duplicates due to SCAN's implementation in Redis.
   for (var i = 0; i < resultKeys.length; i++) {
   console.log(resultKeys[i]);
   }
});
stream.on("end", function() {
   console.log("all keys have been visited");
});
-----------------------------------------------------------------------
There are also hscanStream, zscanStream and sscanStream to iterate through elements in a hash, 
zset and set. The interface of each is similar to scanStream except the first argument is the 
key name:
-----------------------------------------------------------------------
var stream = redis.hscanStream("myhash", {
  match: "age:??"
});
-----------------------------------------------------------------------
ioredis type:
https://www.npmjs.com/package/@types/ioredis
ioredis commmands:
https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/types/ioredis/index.d.ts
-----------------------------------------------------------------------

HTTP Cookies:

document.cookie = "name=Flavio; max-age=3600"; //expires in 1 hour
document.cookie = "name=Flavio; max-age=86400"; //expires in 1 day
document.cookie = "name=Flavio; max-age=2592000"; //expires in 1 month
document.cookie = "name=Flavio; max-age=31536000"; //expires in 1 year

// If you don’t set a path, it defaults to the current document location.
// This means that to apply a global cookie from an inner page, you need to
// specify path="/".
document.cookie = 'name=Flavio; path="/dashboard"; max-age=31536000'; //expires in 1 month

document.cookie = 'name=Flavio; domain="mysite.com";';
// If not set, it defaults to the host portion even if using a subdomain (if on
// subdomain.mydomain.com, by default it’s set to mydomain.com). Domain cookies
// are included in subdomains.

document.cookie = "name=Flavio; Secure; HttpOnly";
// One useful parameter is HttpOnly, which makes cookies inaccessible via the document.
// cookie API, so they are only editable by the server

// To update the value of a cookie, just assign a new value to the cookie name:
document.cookie = "name=Flavio2";

// Similar to updating the value, to update the expiration date, reassign the value with
// a new expires or max-age property:
document.cookie = "name=Flavio; max-age=31536000"; //expires in 1 year
// Just remember to also add any additional parameters you added in the first place,
// like path or domain.

// To delete a cookie, unset its value and pass a date in the past:
document.cookie = "name=; expires=Thu, 01 Jan 1970 00:00:00 UTC;";
// (and again, with all the parameters you used to set it)

// To access a cookie, lookup document.cookie:
const cookies = document.cookie;
// This will return a string with all the cookies set for the page, semicolon separated:
"name1=Flavio1; name2=Flavio2; name3=Flavio3";

// Check if a cookie exists
//ES2016
if (
   document.cookie.split(";").filter(item => {
      return item.includes("name=");
   }).length
) {
   //name exists
}
res.json({ msg: "Users works" });
-----------------------------------------------------------------------
https://flaviocopes.com/cookies/
http://expressjs.com/en/api.html#res.cookie
https://nodejs.org/dist/latest-v8.x/docs/api/http.html#http_response_setheader_name_value
https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies
https://www.npmjs.com/package/cookie
https://www.npmjs.com/package/js-cookie
